{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "SLpbwNePN4zs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]  # 2nd input token is the query\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
        "attn_scores_2_ = query @ inputs.T\n",
        "print(attn_scores_2)\n",
        "print(attn_scores_2_)"
      ],
      "metadata": {
        "id": "xfGj6SDBN4wy",
        "outputId": "e9964b80-4c61-433d-f5ad-d685c22f5e1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum())"
      ],
      "metadata": {
        "id": "2a-AU5oUN4t8",
        "outputId": "a0f66b3f-7872-409d-a6f1-74454fbd848a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# context_vec_2 = torch.zeros(query.shape)\n",
        "# for i,x_i in enumerate(inputs):\n",
        "#     context_vec_2 += attn_weights_2[i]*x_i\n",
        "context_vec_2= attn_weights_2 @ inputs\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "id": "CT4LnVv3N4rG",
        "outputId": "1abdeeb1-82fe-48b9-fbd8-99bded4491be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Scaled dot-product attention**"
      ],
      "metadata": {
        "id": "ipk_O6ABaGKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most notable difference is the introduction of weight matrices that are updated during model training.\n",
        "These trainable weight matrices are crucial so that the model (specifically, the attention module inside the model) can learn to produce \"good\" context vectors"
      ],
      "metadata": {
        "id": "RqW3TOTradXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implementing the self-attention mechanism step by step, we will start by introducing the three training weight matrices $W_q$,$W_k$ and $W_v$\n",
        "\n",
        "*These three matrices are used to project the embedded input tokens,\n",
        " into query, key, and value vectors via matrix multiplication:"
      ],
      "metadata": {
        "id": "eOivLdSoauKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These three matrices are used to project the embedded input tokens,  into query, key, and value vectors via matrix multiplication:\n",
        "\n",
        "Query vector: $q^i=W_q *x^i $ \\\\\n",
        "Key vector: $k^i=W_k *x^i $ \\\\\n",
        "Value vector: $v^i=W_v *x^i $ \\\\"
      ],
      "metadata": {
        "id": "lAasnSnwcg9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = inputs[1] # second input element\n",
        "d_in = inputs.shape[1] # the input embedding size, d=3\n",
        "d_out = 2"
      ],
      "metadata": {
        "id": "vuKSK-NQN4oL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "torch.manual_seed(123)\n",
        "\n",
        "W_query = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ],
      "metadata": {
        "id": "b7OwrSrhN4lS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "anMOOhA8lqDG",
        "outputId": "b0b0360b-bfc3-4ecd-ab3d-c4933e77f78c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551]) torch.Size([2])\n"
          ]
        }
      ],
      "source": [
        "query_2 = x @ W_query # _2 because it's with respect to the 2nd input element\n",
        "key_2 = x @ W_key\n",
        "value_2 = x @ W_value\n",
        "\n",
        "print(query_2,query_2.size())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ],
      "metadata": {
        "id": "tZrKB6okdzz3",
        "outputId": "f54f731f-feb1-4809-dec9-dbe9a1a98476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we compute the attention weights by computing the dot product between the query and each key vector:"
      ],
      "metadata": {
        "id": "i5TxJr5IeFeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_score= query_2.unsqueeze(dim=0) @ keys.T\n",
        "print(attn_score)\n",
        "attn_weights = torch.softmax(attn_score/ d_out**0.5,dim=1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "id": "GoWodJw9dzw3",
        "outputId": "7471ec74-9fef-4ad5-f6ce-b1a67af171c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440]])\n",
            "tensor([[0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = attn_weights @ values\n",
        "context_vec_2"
      ],
      "metadata": {
        "id": "WO5BK9lLdzt0",
        "outputId": "9dfd2a9f-8264-42ec-91ec-5022b068caf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3061, 0.8210]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing a compact SelfAttention class"
      ],
      "metadata": {
        "id": "XPJ05KFEfg9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / self.d_out**0.5, dim=1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(789)\n",
        "sa = SelfAttention(d_in, d_out)\n",
        "print(sa(inputs))\n"
      ],
      "metadata": {
        "id": "SQh_p0A6dzqy",
        "outputId": "7cc68893-1c8d-40f4-9b8d-a632a3915e70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Causal self-attention"
      ],
      "metadata": {
        "id": "4SqTGUPfgpNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causal self-attention ensures that the model's prediction for a certain position in a sequence is only dependent on the known outputs at previous positions, not on future positions.\n"
      ],
      "metadata": {
        "id": "OVAmW8d8grnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's start with some attention weights\n",
        "attn_weights=torch.tensor([[0.1972, 0.1910, 0.1894, 0.1361, 0.1344, 0.1520],\n",
        "        [0.1476, 0.2164, 0.2134, 0.1365, 0.1240, 0.1621],\n",
        "        [0.1479, 0.2157, 0.2129, 0.1366, 0.1260, 0.1608],\n",
        "        [0.1505, 0.1952, 0.1933, 0.1525, 0.1375, 0.1711],\n",
        "        [0.1571, 0.1874, 0.1885, 0.1453, 0.1819, 0.1399],\n",
        "        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])\n",
        "attn_weights"
      ],
      "metadata": {
        "id": "kPdJF9AKdznM",
        "outputId": "67b7e604-65d3-471c-adc8-47272baf6342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1972, 0.1910, 0.1894, 0.1361, 0.1344, 0.1520],\n",
              "        [0.1476, 0.2164, 0.2134, 0.1365, 0.1240, 0.1621],\n",
              "        [0.1479, 0.2157, 0.2129, 0.1366, 0.1260, 0.1608],\n",
              "        [0.1505, 0.1952, 0.1933, 0.1525, 0.1375, 0.1711],\n",
              "        [0.1571, 0.1874, 0.1885, 0.1453, 0.1819, 0.1399],\n",
              "        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = attn_weights.size(0)\n",
        "mask_simple = torch.tril(torch.ones(block_size, block_size))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "id": "h-h_8JD9dzkR",
        "outputId": "0d9ccb3d-7ae3-4d9b-9b5d-73223b3dd7f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#multiply the attention weights with this mask to zero out the attention scores above the diagonal:\n",
        "masked_simple = attn_weights*mask_simple\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "id": "G-VzJy2vdzhH",
        "outputId": "7285dfb1-df5c-4199-d939-72a74f5d8537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1972, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1476, 0.2164, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1479, 0.2157, 0.2129, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1505, 0.1952, 0.1933, 0.1525, 0.0000, 0.0000],\n",
            "        [0.1571, 0.1874, 0.1885, 0.1453, 0.1819, 0.0000],\n",
            "        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "id": "OQ9VHWOPdzd8",
        "outputId": "7b5a42db-088b-4372-e0b0-be427a92706a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4055, 0.5945, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2565, 0.3742, 0.3693, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2176, 0.2823, 0.2795, 0.2205, 0.0000, 0.0000],\n",
            "        [0.1826, 0.2179, 0.2191, 0.1689, 0.2115, 0.0000],\n",
            "        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, instead of zeroing out attention weights above the diagonal and renormalizing the results, we can mask the unnormalized attention scores above the diagonal with negative infinity before they enter the softmax function"
      ],
      "metadata": {
        "id": "e-fh8L6ijEDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
        "# masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "# print(masked)\n",
        "# attn_weights = torch.softmax(masked / d_out**0.5, dim=1)\n",
        "# print(attn_weights)"
      ],
      "metadata": {
        "id": "KwmiBwODjDzf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Masking additional attention weights with dropout** \\\\\n",
        "In addition, we also apply dropout to reduce overfitting during training.\n",
        "\n",
        "we will apply the dropout mask after computing the attention weights because it's more common."
      ],
      "metadata": {
        "id": "UBgw2Gqejith"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
        "example = torch.ones(6, 6) # create a matrix of ones\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "id": "WjU61xCxjDwi",
        "outputId": "e0cb76ce-0ba7-4a5a-c2be-634330d02c6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_dropout = dropout(attn_weights)\n",
        "attn_dropout"
      ],
      "metadata": {
        "id": "0ZS5j5U4jDth",
        "outputId": "3c9c7913-c495-42ee-aad5-9ae93467b314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3944, 0.3820, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.4268, 0.0000, 0.0000, 0.3242],\n",
              "        [0.0000, 0.0000, 0.4258, 0.2732, 0.0000, 0.3216],\n",
              "        [0.0000, 0.3904, 0.0000, 0.3050, 0.2750, 0.0000],\n",
              "        [0.0000, 0.3748, 0.3770, 0.2906, 0.3638, 0.2798],\n",
              "        [0.2946, 0.4066, 0.0000, 0.0000, 0.2320, 0.3678]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalized attn_weights after dropout\n",
        "sum_weights=torch.sum(attn_dropout, dim=1, keepdim=True)\n",
        "attn_dropout=attn_dropout/sum_weights\n",
        "attn_dropout"
      ],
      "metadata": {
        "id": "pAgN2XgDjDqk",
        "outputId": "7c01fb28-1430-4955-c0bd-bb9c09623be7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5080, 0.4920, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.5683, 0.0000, 0.0000, 0.4317],\n",
              "        [0.0000, 0.0000, 0.4172, 0.2677, 0.0000, 0.3151],\n",
              "        [0.0000, 0.4023, 0.0000, 0.3143, 0.2834, 0.0000],\n",
              "        [0.0000, 0.2223, 0.2236, 0.1724, 0.2158, 0.1660],\n",
              "        [0.2264, 0.3125, 0.0000, 0.0000, 0.1783, 0.2827]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One more thing is to implement the code to **handle batches** consisting of more than one input so that our CausalSelfAttention class supports the batch outputs"
      ],
      "metadata": {
        "id": "V6CTV--SmfNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs, inputs), dim=0)\n",
        "print(batch.shape) # batch with size 3"
      ],
      "metadata": {
        "id": "TuaT5BkJjDnl",
        "outputId": "0b9a5139-cf01-4703-cddf-c3aecd2b4b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.mask=torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
        "        # self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n_tokens, d_in = x.shape # New batch dimension b\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "        attn_scores = torch.bmm(queries,keys.permute(0,2,1)) #queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        attn_scores.masked_fill_( self.mask.bool()[:n_tokens, :n_tokens], -torch.inf)\n",
        "        attn_weights = torch.softmax(attn_scores / self.d_out**0.5, dim=1)\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "block_size = batch.shape[1]\n",
        "csa = CausalSelfAttention(d_in, d_out, block_size, 0.0)\n",
        "\n",
        "context_vecs = csa(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "id": "RJikteQgjDko",
        "outputId": "abb39a3f-83ae-4813-e6e6-0483e291d399",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0844,  0.0414],\n",
            "         [-0.2264, -0.0039],\n",
            "         [-0.4163, -0.0564],\n",
            "         [-0.5014, -0.1011],\n",
            "         [-0.7754, -0.1867],\n",
            "         [-1.1632, -0.3303]],\n",
            "\n",
            "        [[-0.0844,  0.0414],\n",
            "         [-0.2264, -0.0039],\n",
            "         [-0.4163, -0.0564],\n",
            "         [-0.5014, -0.1011],\n",
            "         [-0.7754, -0.1867],\n",
            "         [-1.1632, -0.3303]],\n",
            "\n",
            "        [[-0.0844,  0.0414],\n",
            "         [-0.2264, -0.0039],\n",
            "         [-0.4163, -0.0564],\n",
            "         [-0.5014, -0.1011],\n",
            "         [-0.7754, -0.1867],\n",
            "         [-1.1632, -0.3303]]], grad_fn=<UnsafeViewBackward0>)\n",
            "context_vecs.shape: torch.Size([3, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note that dropout is only applied during training, not during inference.**"
      ],
      "metadata": {
        "id": "elXwguDJpnc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extending single-head attention to multi-head attention"
      ],
      "metadata": {
        "id": "AoyuC7bZqDQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections. This allows the model to jointly attend to information from different representation subspaces at different positions."
      ],
      "metadata": {
        "id": "D1HjN-SgqSeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, block_size, dropout, num_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalSelfAttention(d_in, d_out, block_size, dropout)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "block_size = batch.shape[1]\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "id": "-kV3soOFjDgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e11da09-8ef6-468b-8c2e-b3f6cf5b39de"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0844,  0.0414,  0.0766,  0.0171],\n",
            "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
            "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
            "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
            "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
            "         [-1.1632, -0.3303,  1.1224,  0.8460]],\n",
            "\n",
            "        [[-0.0844,  0.0414,  0.0766,  0.0171],\n",
            "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
            "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
            "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
            "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
            "         [-1.1632, -0.3303,  1.1224,  0.8460]],\n",
            "\n",
            "        [[-0.0844,  0.0414,  0.0766,  0.0171],\n",
            "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
            "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
            "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
            "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
            "         [-1.1632, -0.3303,  1.1224,  0.8460]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([3, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More efficient implementation** \\\\\n",
        "While the above is an intuitive and fully functional implementation of multi-head attention (wrapping the single-head attention CausalSelfAttention implementation from earlier), we can write a stand-alone class called MultiHeadAttention to achieve the same.\n",
        "\n",
        "We don't concatenate single attention heads for this stand-alone MultiHeadAttention class. Instead, we create single W_query, W_key, and W_value weight matrices and then split those into individual matrices for each attention head:"
      ],
      "metadata": {
        "id": "NmVpm5bftee0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, block_size, dropout, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n_tokens, d_in = x.shape\n",
        "        # (b, n_heads, T) -> (b, T, n_heads, head_dim)\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, T, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitely split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, T, d_out) -> (b, T, num_heads, head_dim)\n",
        "        keys = keys.view(b, n_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, n_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, n_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, T, num_heads, head_dim) -> (b, num_heads, T, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "        attn_scores.masked_fill_(self.mask.bool()[:n_tokens, :n_tokens].unsqueeze(0).unsqueeze(0), -torch.inf)\n",
        "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2) # Shape: (b, T, n_heads, head_dim)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, n_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, block_size, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads=2)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guzurDIoqV-z",
        "outputId": "1af84725-4da1-43a2-f428-c8d63aca84d8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([3, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4JPqWA4ljDdt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}